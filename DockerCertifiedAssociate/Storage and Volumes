Compare and Contrast Object and Block storage and when they should be used:
    - Ideally, very little data is written to a cntainer's wriatebl layer, and you use the Docker volumes to write data. 
    - Some workloads require you to be able to write to the container's wraitbel layer, this is where storage drivers come in. 
    - Docker supports several different storage drivers, using a pluggable architecture. 

    * About Storage drivers
        - Storage drivers allow you to create data in the writable layer of your container, the files won't be persisted after the container os deleted
        - The read and write speeds are also lower than the native filesystem performance 
        - Each layer in a docker image is read only, except for the last one 
            -> Each layer is only the set of differences that come before it 
        - When you create a nwe conainer, you add a new writabele layer on top of the underlyig layers, this is called the conatiner layer 
        - The major diff between containers and images is the top writable layer 
        - Copy on Write Strategy
            -> for max efficiency, if a file in a lower layer in the image and another layer needs access to it, it just uses the existing file. 
            -> The first time the upper layer needs to modify the file, the file is copied into that layer and modified
            -> This is to minimize I/O and reduce layer size 
        - The top writable layer is as small as possible 
        - Whne an existing file in a container is modified, the storage driver performs a cCOW operation
        - Storage drivers include: aufs, overlay, overlya2 
    
    - Which storage driver to use? 3 Different factors to consider 
        - If multiple storage drivers are suppoorted in your kernel, Docker hasa prioritised list of which storage driver to use if no 
            storage driver is explicitly configured
        - Use the storage driver with the best overall performance and stabability in most usual scenarios 

    - When possible, overlay2 is the recommended storage driver, it is often the default storage driver 
    - When in doubt, use a moder linux ditro with a kernel that supports the voerlay2 storage drivers, and use Docker volumes for write-heavy workloads
    - Other considerations 
        -> Overlay2, aufs, overlay all operate at the file level rather than the block level, this uyses memory more efficiently, but in heavy workloads,
            the containers writable level with grow quite large
        -> Block level storage drivers like devicemapper, btrfs perform better for write heavy workloads 
        -> For small containers with many layers or deep file systems, overlay will perform better than overlay2 
        -> Highest Stabability: overlay2, overlay, auf, devicemapper 
    - To check your storage driver: $ docker info

Demonstrate how storage can be used across cluster nodes 
    - This is possible with Volume Plugins, which can provide persistent storage across a cluster 
    - A network plugin might provide network plumbing 

Demonstrate how to configure devicemapper       
    - thin pool: a storage souce that provides on-demand allocation for storage space (similar to virtual memory)
    - Device Mapper is a kernel-based framework that underpins many advanced volume management technologies 
    * Configure Docker w the devicemapper storage driver 
        -> Configure loop-lvm mode for testing 
            - This mode is onlny for testing, the loopback mechanism allows files on the local disk to be read from and written
                from as if they were an actual physical disk of block device, however, IO will be slower, and can have race conditions 
            - Still, its usefyl for debugging 
        -> To do so, stop docker, edit /stc/docker/daemon.json, and insert storagedriver as devicemapper, then restart docker 
        -> Production hosts using the devicemapper, must use direct-lvm as mode, this mode uses block drvices to create the thin pool, 
            its faster than the loopback (loop-lvm) mode, 
        -> Docker direct-lvm mode is easy enough to do if you're starting it for the first time, but not supported after docker has prepared the block  
            device for you 
        -> The manual proceedure is long 
            1) Identify the blaock you want to use
            2) Stop docker 
            3) install lvm2, device-mapper
            4) Create a physical volume on your block device 
            5) Create a docker volume on the same device 
            6) Create two logical volumes 
            7) Convert the logical volumes to a thin pool and a storage location for metadata for the thin pool 
            8) Configure autoextenion of thin pools via an lvm profile 
            9) Configure autoextenion of thin pools via an lvm profile 
            10) Apply the LVM profile using lvchange 
            11) Ensure monitoring of he logical volume is enabled 
            12) Start Docker 
            13) docker info
    - Manage devicemapper
        - Monitor the thin pool: Use nagios, don't use the LVM autoextension alone. You can monitor free space using lvs -a 
        - TO view LVM logs, use $ sudo journalctl -fu dm-event.service 
    - Increase Capacity on a running device
        - Resize loop-lvm 
            - device_tool_tulilty 
                $ ./device-tool resize 200GB 
            - Operating system utiliities 
                1) truncate -s new size /var/lib/docker.devicemapper/data 
                2) Reload it  
        - Resize a divert-lvm thin pool 
            - To extend a direc-lvm, you need to first attach a new block deive to the docker host, 
                1) gather info about your volume group 
                2) Extend the volume group 
                3) Extend the docker/thinpool logical volume 
                4) verify the new thin pool size 
    - Activate devicemapper after reboot 
        - If you reboot host an find docker service failed to start, look for the error "non existing device" 
            1) You need to reactivate the logical volumes with this commend 
                $ sudo lvchange -ay docker-thinpool 
    - How devicemapper works 
        -> Uses dedicated block devices rather than formatted filesystems and operates on files at the block level for max performance for 
            copy on write operations 
        -> Uses snapshots, which stores the differences introduced in each layer as very small, lightweight thin pools 
    - Devicemapper workflow 
        - All objects are store din /var/lib/docker/devicemapper     
        - reads happen at the block level 
        - writing happens by the "allocate on demand" operation 

Describe how volumes are used with Docker for persistent storage 
    - Volumes are the preferred mechanism for persisting data generated and used by Docker containers 
    - Whilst bind mounts are dependent on the directory structure of the OS and the host machine, volumes are completely manage dby Docker
    - Volumes vs bind mounts:
        1) Volyes are easier to back up than bind mounts
        2) You can manage volymnes ysing Docker CLI commands or the Docker API
        3) Volymes work on boath Linux and Windos containers 
        4) Volumes can be more safely shared among multiple containers 
        5) Volume drivers let you store volumes on remote hosts or ckloud providers, to encrypt the contents of volyeor to add vfunctionaity 

    - Volumes are also a better shoice than eprsisting in a container's writable layer, because a volume does not increase the sixze of the
        containers using it, and the volume's contents exist outside the lifecycle of a given container 
    - Three optinos to share files between host machine n container, once a container is stopped, volumes, bind mounts persist. tmpfs don't and go when container dies

    * Choose the -v or the --mount flag 
        -v nameofthevolume:/thepath:options 
        --mount <--- this consists of multiple key value pair s
        when using volumes as services, only --mount is supported 
    
    Create and manage Volumes   
        - Unlike a bind mount, you can create and manage volumes outside the scope of any container 
            $ docker volume create  {name} 
            $ docker volume ls 
            $ docker volume insepct  {name}
            $ docker volume rm {name}
    Starting and managing a container with a volume 
        $ docker run -d --name test --mount source=myvol,target=/app nginx:latest
        $ docker run -d --name tes -v name:/app nginx:latest
    
    Docker Compose and Volumes
        - Simpley add it in the volumes section of the yaml
        - On the first invocation of docker-compose up, the volume will be created, from then onwards, it will be reused 
        - A volume can be craeted outside of docker compose, and then referenced inside the docker-compose yaml 
            -> It just has to be listed as "external" in the yaml file 
    
    Start a Service with volumes: 
        - When you start a service and define a volume, each service vontainer users its own local volume 
        - None of the coantiners can share this data if you use the local volume driver, but some volue drivers do support shared storage
    
    Read only volume
        - Much the same way craeted as normal, but pass it the ro option 
        - $ docker run -d --name test --mount nginx:/mypath:ro nginx
    
    Share Data among machines:
        - For fault tolerant applictinos, you might need to configure multiple replicas of the same service to have access to the same files 
        - There are two ways to do this
            1) Setup some sorta cloud object storage system like amazon s3 
            2) External storage system like NFS 
        - Volume drivers allow you to abstract the underlying storage systemf rom the application logic 
            -> if you services use a volume with an NFS driverm you can update the services to use a different driver 
        
        - Use a volume driver 
            Whjen you crate a volume usig docker volume crate, or when you start a container which uses a not yet created volume, you can specify a volume drive required

        Backup, restore, or migrate data volumes 
            -  --volumes-from flag to craete a new conatiner that mounts that volume 
    
    Removing:
        $ docker run --rm  -v 
        $ docker volume prune 

Identiy the steps you would to take to clean up unused images on a file system also on DTR 
    - Prune unused docker object storage
        by defualt docker prune only cleaned up dangling issues
            -> A dangling image is one that is not tagged or refeerenced by any container 
            -> $ docker image prune 
        - To remove all images which are not used by existing container, -a flag 
            $ docker image prune -a 
        - You can use the fliter flag as well --filter="until=24h"
        - when you stop a container it is not automatically rmeoved unless you used the --rm flat 
        to see all conatiner on the Docker host, included stopped containers, use $ docker ps -a
        - Prune Volumes the same way x 
        - Prune networks the same way 
        - Prune everything: docker system prune 
            -> Volumes are not pruned by default, and you must use the docker system prune --volumes 
    - DTR Garbage Collection 

State which graph driver should be used on which OS

Summarise How an Application is Composed of layers and where those layers reside on the filesystem
    * About storage drivers
        - TO use storage drivers effecively, its important to know how Docker builds and stores images, and how these images are used by caontienrs 
        - Storage drivers allow you to create data in the writable layers of your conatiner
            -> They fiules won't be persisted adfter the container is deleted