Networking
    - Describe the container model and how it interfaces with the Docker engine and network IPAM drivers 
        * The Container networking model 
            - Networking architecture is built on a set of interfaces, called a container netowrkring model 
                -> provide portabilitu across diverse infrastructures 
            - Path:
                network infrastructure -> Network Driver -> Docker Enginer -> Netowkr -> Endpoint -> container 
            - Container Network Model Constructs
                1) Sandbox
                    Conatins the network stack, the manageemnt of the containers interfaces, routing table, and DNS settings 
                2) Endpoint
                    An endpoint joins a sandbox to a network. The endpoint exists to abstract away the connection details 
                    and makes this more portable.
                3) Network
                    A network is a collection of endpoints that have connectivity between them. CNM does not specify a netowkring in 
                    terms of the OSI model. An implementation could be a linux bridge, a VLAN etc 
            - Container Network Model Driver Interfaces 
                -> Provides two pluggable and open interfaces that can be controlled by users 
                1) Network Drivers 
                    - Docker Network Drivers provide the actual implemntation that makes netowks work.
                    - They are pluggable so that idfferent drivers can eb used and interchanged easily to support diff use causes
                        1) Native netowk drivers:
                            -> They are a native part of the docker engine and are provided by Docker
                            -> Muliple drivers to choose from, and they support diff abilities like overlap=y networks and local bridges 
                        2) Remote Network Drivers: 
                            -> Drivers created by the community 
                            -> Provide intergratino with other software 
                2) IPAM Drivers 
                    -> Docker has a native IP Address Management Driver that provides default subnets or IP addresses for the netowkrs and endpoints 
                        if they are not specified. 
                    -> IP Addressing can also be manually assigned throuhg netowkr, container, and service create commands 
                    -> Remote IPAM drivers also exist and provide integration to IPAM tools 
            - Docker Native Network Drivers 
                1) Host 
                    Wiht the host driver, the container use the netowking stack of the host
                    There is no namespace separation and all the interfaces on the host can be used directly by the contaienr 
                2) Bridge 
                    Creates a Linux bridge on the host that is managed by Docker
                    Containers on a bridge can communicate with each other 
                    External access can also be configured by a bridge driver 
                3) Overlay 
                    An overlay driver creates a network which supports multi host netowkrs out of the box
                    uses a combonation of VLANS and Briges to acheive this 
                4) MACVLAN 
                    Uses the Lniux LACVLAN bridge to establish a connectino between conainer interfaces and partent host interface 
                    Can be used to provide IP addressees to containers that are routable on the physical network 
                5) None 
                    The none driver gives a conatiner its own netowrking stack and netowk namespace, but does not configure interfaces inside the container 
            - Network Scope
                $ docker network ls 
                Scope can either be local or swarm 
                Local provides connectivity and netowkr services within the scrope of a host 
                Swarm drivers provide connectvity and network srevices across a swarm cluster 
                Swarm Scope network have the same network id 
            - Docker Remote Network Drivers 
                1) Contriv 
                    - An open source plugin lead by Cisco Systems to provide infrastructure and security to multi tennant deployments 
                2) Weave 
                    - A network plugin that creates a virtual network that connects Docker conainers across multiple hosts or clouds 
                3) kuryr 
                    Utilizes Neutron 
            - Docker Remote IPAM Drivers 
                -> infoblox, an open source IPAM plugin that provides integratino with existing infoblox tools 

    - Describe the different use cases fr the built-in network drivers 
        1) Bridge Network Driver 
            - easy to use, good for developers new to docker 
            - Creates a private network internal to the host, so conainers on this netowrk can communicate.
            - External access is granted by exposting ports to containers 
            - Conainers added to this netowrk, can communicate with each other by using their conatiner naems 
        2) Overlay Network Driver
            - Radically simplifies many of the complexities in multihost networking 
            - It is a swarm scope driver, which means that it operates across an entire SWARM or UCP cluster rather than individual hosts 
            - With the overlay driver, multi-host netowrks are firs class citizens inside docker without provisinoiing or components 
            - IPAM, Service Discovery, Multihost connectivity, load balancing, and encryption are built right in 
            - The overlay driver decouples the conatinner netwrork from the underlyiny physical netowwrk.
                -> This gives maximum portabliity 
                -> Network policy, vicibilitily and security is controlled centrally through the Docker Universal Control Plans 
            - An out of the box solutino which scales well 
        3) MACVLAN Host 
            - Newest driver 
            - Very lightweight 
            - It connects conatiner intrefaces directly to host interfaces 
            - Containers are addressed with routable IP addresses that are on the subnet of the external network 
            - Can communicate with resources outside the SWARM cluster without the use of NAT and portmapping 
                -> This aids network visibility and torubleshooting 
                -> Additionally the direct traddic ath between conatiners and the host interfaces reduces latency 
        
        - Userdefined bridge networks are best when you need multiple container to communicate on the same docker host 
        - Host networks are best when the netwrok stack should not be isolated from ther Docker host, but you want other aspects of the conatiner to be isolated
        - Overlay netowrks are best when you need conatiners running on different Docker hosts to communicate, or when using swarm services
        - Macvlan networks are best when you are migrating from a VM setup or need your containers to look lik ephysical hosts on your netowrk, each with their own mac address


    - Understand and describe the types of traffic that flow between the Docker Enginer Registry and UCP controllers 
        The traffic between UCP and DTR is always encrypted to ensure security.
        Traffic between conainer is not encrypedby default 
        DTR/UCP management traffic used Mutual TLS 

    - Create a Bridge network for a developer to use their containers on
        By default, Docker Engine connects conatiner via a default bridge to the netowkr of the node
        However, only overlay networks in a Swarm conetxt let containers communicate further than the node's netowkr boundaries 
        * Userdefined network bridges and the default bridge 
            - botha  link layer that forwards network traffic between netowrk segment services
            - User defined bridges provide automatic DNS resolution between containers 
                - Containers on the default bridge netwerok can onlu access each other bny IP addresses
                - On a user defined netwrok bridge, containers can access each other by using their name/aliases 
                - userdefined bridges provide better isolation, as it provides a scoped network in which only attached containers can communicate 
            - Containers can be attached and detached from uers-defined netowrks on the fly 
                - During a containers liftime you can connect or disconnect it from user defined networks on the fly
            - Linked netowrks by default share environment variables 
        $ docker network craete my-net 
        $ docker network rm my-net
        $ docker craete --name my-nginx --network my-net --publish 8080:80 nginx:latest
        $ docker network connect my-net my-nginx
        $ docker network disconnect my-net my-nginx
    
    - Publish a port so that an appliction is accessible externally 
        - Multiple ways of doing this
        - can use expose in the Dockerfile 
        - docker run, you can use the parameter -P 
            -> can also do it manually with docker -p 8080:80 
        - You can also lock and randomise ports 

    - IDENTIFY WHICH IP AND PORT A CONATINER IS externally accessible on
        - Basically, you can look up every port which is exposed using $ docker inspect ${container}
        - To get port binding information, you could use $ docker port ${container}
            -> $ docker ps -q (to get all the container ids)
    
    - Compare and Contrast "host" and "ingress" publishing modes 
        * Publish Port on single host in Docker Swarm
            - $ mode=host --publish
        * Publish a service's ports using the routing mesh 
            - To publish a service's ports externally to the swarm, use the --publish <PUBLISHED_PORT>:<SERVICE_PORT> flag 
            - The swarm makes the service accesssible at the published port on every swarm node 
            - If an external host connects to that port on any swarm node, the routing mesh routes it to a task
            - The external host does not need to know the IP Address or interanlly used ports of the service tasks to interact with those services
    
    - Configue Docker to use External DNS 
        - TO set the DNS server for all Docker conainers, use 
            $ sudo dockerd --dns 8.8.8.8
        - To set the DNS search domain for all Docker containers use:
            $ sudo dockerd --dns-search example.com

    - Describe and demonstrate how to use Docker to load balance HTTP/HTTPs traddic to an application 
        * How to configure external load balancer 
            - You can configure an external load balancer for swarm, either in combination with the routing mesh, or without using the routing mesh at all 
            - Using the Routing Mesh 
                -> You can configure a thrid party proxy to route requests to a swarm service 
                -> The swarm nodes can reside on  aprivate network that is accesible by the load balancer, but not publicly 
        * Without the routing mesh  
            - set --endpoint-mode to dnsrr instead of the default vip 
            - In this case, there is not a single virtual ip, instead Docker sets up the DNS entries for the service such that a DNS query for the service
                name returns a list of ipaddresses, and the client connects directly to one of those 
            - You are responsible for providing a list of ip addresses and ports to your load balancer 
    
    - Undertand and describe the different types of traffic between Docker Enginer, UCP, and registry 
        - Traffic between DTR and UCP is always encrypted to ensure security
        - Traffic between containers is not encrypted by default 
        - DTR / UCP management traffic uses Mutual TLS 
    
    - Describe and demonstrate how to deploy a service on a docker overlay network 
        1) Pubish a service's ports using the routing mesh 
            docker service create --name web --replicas 3 --publish published=8080,target=80 nginx
        2) Connect the service to an overlay network 
            -> Create overlay network $ docker network create --driver overlay my-network 
            -> Create a new service and pass the --network flag to connect $ docker service create --replicas 3 --network my-network --name web nginx 
            OR 
            -> Connect to an existing network using the --network-add 
                $ docker service update --network-add my-network my-web
            
            -> Remove a network with 
                $ docker service update --netowkr-rm my-network my-web
    
    - Describe and demonstrate how to troubleshoot container and engine logs to resolve connectivity issues between containers 
        - $ docker network inspect
            ^ Do this to see whether those conatiners are connected through the same network 
            - Also its important if youre running docker swarm, if so your conatnienrs need to be bound together by the overlay network if theyre run 
                on different nodes in the swarm 
            - Bridge netowrks aren't able to communicate over node boundaries 
    
    - Describe how to roue traffic to kubernetes pods using ClusterIp and NodePort services 
        - 
    
    - Describe the kubernetes container network model 
    